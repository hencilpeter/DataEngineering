1. data structure reference 
	https://www.techinterviewhandbook.org/algorithms/study-cheatsheet/
	https://www.interviewcake.com/data-structures-reference
	https://cheatography.com/burcuco/cheat-sheets/data-structures-and-algorithms/pdf/

2. sorting 
	https://visualgo.net/en/sorting?slide=1-1     - good one with animation 
	https://www.interviewcake.com/sorting-algorithm-cheat-sheet
	https://algs4.cs.princeton.edu/22mergesort/
	
Algorithms
	https://www.techinterviewhandbook.org/algorithms/study-cheatsheet/
	
[1]Datastrucrures:
1.1. Array : Arrays hold values of the same type at contiguous memory locations. In an array, we're usually concerned about two things - the position/index of an element and the element itself.
     Advantages-Accessing elements is fast as long as you have the index
  	 Disadvantages - Addition and removal of elements into/from the middle of an array is slow because the remaining elements need to be shifted to accommodate the new/missing element. An exception to this is if the position to be inserted/removed is at the end of the array.
	 Time Complexity : Access - O(1) , insert/search/remove - O(n)
1.2. String	: A string is a sequence of characters.
	datastructure for string  : (i) Trie/Prefix Tree (ii) Suffix Tree
	algorithm for string : (i) Rabin Karp (ii) KMP 
	Time Complexity : Access - O(1) , insert/search/remove - O(n)
1.3. Stack (abstract datatype)
	Stacks implements LIFO behaviour. i.e. set of physical items stacked on top of each other. supports the operations push (insert a new element on the top of the stack) and pop (remove and return the most recently added element, the element at the top of the stack). 
	Stacks are an important way of supporting nested or recursive function calls and is used to implement depth-first search. Depth-first search can be implemented using recursion or a manual stack.
	Time Complexity : Top (or)Peek/push/pop/isempty - O(1) , search - O(n)
1.4. Queue (abstract datatype)
	Queue implements FIFO behaviour. similar to people lining up in real life to wait for goods or services.
	A queue is a linear collection of elements that are maintained in a sequence and can be modified by the addition of elements at one end of the sequence (enqueue operation) and the removal of elements from the other end (dequeue operation). 
	 - end of the sequence at which elements are added is called the back, tail, or rear of the queue
	 - end at which elements are removed is called the head or front of the queue
	 queues can be implemented using arrays or singly linked lists.
	 Time Complexity : enqueue/dequeue/isEmpty/front/back- O(1) 
1.5. Linked List
	Linked list is used to represent sequential data. It is a linear collection of data elements whose order is not given by their physical placement in memory, as opposed to arrays, where data is stored in sequential blocks of memory. Instead, each element contains an address of the next element. It is a data structure consisting of a collection of nodes which together represent a sequence. In short, each node contains: data, and a reference (in other words, a link) to the next node in the sequence.
	Advantages: insertion/deletion of a node is O(1)
	Disadvantages: access O(n)
	type of linked list: 
	  singly linked list - each node points to the next node and last node  next points to NULL
	  doubly linked list - linked list with two pointers (next and prev). last node's next points to NULL
	  circular linked list  - a singly linked list where last node points back to the first node. 
	  circular doubly linked list - where the prev pointer of the first node points to the last node and the next pointer of the last node points to the first node.
1.6. Hash Table
	A Data structure that can map keys to values. A hash table uses a hash function on an element to compute an index, also called a hash code, into an array of buckets or slots, from which the desired value can be found. During lookup, the key is hashed and the resulting hash indicates where the corresponding value is stored.
	
	Time complexity: Search/Insert/Remove - O(1)
	collection problem's solution: (i) chaining (linked list used to store the collided items)  (ii) open addressing (all entry records are stored inthe bucket array itself. when new entry to be inserted, buckets are examined, starting with the hashed-to slot and look fo rthe unoccupied slot is found) 
	
	Additional nodes: 
	-----------------
	(i) from keys to indices: maping of keys to indices of a hash table called a hash function:
         hash function is the combination of (i) hash-code maps  and (ii) compression maps 	
		 hash code map : key -> integer 
		 compression map : integer -> [0, N-1]
	(ii) Popular hash-code maps : (i) integer cast (ii) component sum (iii) polynomical accumulation 
	(iii) compression maps: use remainder
	(iv) choosing hash table size : (i) use prime number (ensure uniform distribution and prime no. should be close to exact power of 2)  
	                                 (ii) given the number of elements (n), choose the reasonable prime number with acceptable no. of collision  (e.g. 2 to 4 
	(v) choosing good hash function: (i) quick to compute (ii) distribtue keys uniformly though out the table (iii) minimize the collisions 
	(vi) hasing non-integer keys: convert to ascii values of characters then use standard hash function on the integer.  
	(vii) open addressing - (i) linear probing (ii) double hashing 

1.7. Tree	
Set of connected nodes. Each node in the tree can be connected to many children, but must be connected to exactly one parent, except for the root node, which has no parent. A tree is an undirected and connected acyclic graph. There are no cycles or loops. Each node can be like the root node of its own subtree, making recursion a useful technique for tree traversal.

Trees are commonly used to represent hierarchical data, e.g. file systems, JSON, and HTML documents. 

1.7.1 Binary tree
Binary means two, so nodes in a binary trees have a maximum of two children.
Complete binary tree - A complete binary tree is a binary tree in which every level, except possibly the last, is completely filled, and all nodes in the last level are as far left as possible.
Balanced binary tree - A binary tree structure in which the left and right subtrees of every node differ in height by no more than 1.
Tree Traversal:
	In-order traversal - Left -> Root -> Right
	Result: 2, 7, 5, 6, 11, 1, 9, 5, 9
	Pre-order traversal - Root -> Left -> Right
	Result: 1, 7, 2, 6, 5, 11, 9, 9, 5
	Post-order traversal - Left -> Right -> Root
	Result: 2, 5, 11, 6, 7, 5, 9, 9, 1
Binary search tree (BST)
 In-order traversal of a BST will give you all elements in order.
 Time complexity : Access/Search/Insert/Remove - O(log(n)) 
 
1.7.2 AVL Tree 
	An AVL tree is a type of binary search tree. AVL trees have the property of dynamic self-balancing in addition to all the other properties exhibited by binary search trees.
	use case: AVL trees are mostly used for in-memory sorts of sets and dictionaries. AVL trees are also used extensively in database applications in which insertions and deletions are fewer but there are frequent lookups for data required
1.7.3 Red Black 
	A red-black tree is a kind of self-balancing binary search tree where each node has an extra bit, and that bit is often interpreted as the color (red or black). These colors are used to ensure that the tree remains balanced during insertions and deletions. Although the balance of the tree is not perfect, it is good enough to reduce the searching time and maintain it around O(log n) time
	use cases: Real-world uses of red-black trees include TreeSet, TreeMap, and Hashmap in the Java Collections Library
1.8. Graph	
	containing a set of objects (nodes or vertices) where there can be edges between these nodes/vertices. 
	Edges can be directed or undirected and can optionally have values (a weighted graph). 
	Trees are undirected graphs in which any two vertices are connected by exactly one edge and there can be no cycles in the graph.
	Graph representation:
	(i) Adjacency matrix (ii) Adjacency list (iii) Hash table of hash tables
	Time complexity:
	  Depth-first search	O(|V| + |E|)
	  Breadth-first search	O(|V| + |E|)
	  Topological sort	O(|V| + |E|)
1.9. Heap	
	A heap is a specialized tree-based data structure which is a complete tree that satisfies the heap property.
	Max heap - In a max heap the value of a node must be greatest among the node values in its entire subtree. The same property must be recursively true for all nodes in the tree.
	Min heap - In a min heap the value of a node must be smallest among the node values in its entire subtree. The same property must be recursively true for all nodes in the tree.
	Time Complexity:
		Find max/min	O(1)
		Insert	O(log(n))
		Remove	O(log(n))
		Heapify (create a heap out of given array of elements)	O(n)
	application: Priority Queue 
1.10. Trie	
	Tries are special trees (prefix trees) that make searching and storing strings more efficient. Tries have many practical applications, such as conducting searches and providing autocomplete. 
	use case : string auto complete
	
	Time Complexity: Search/Insert/Remove - O(m) 
	
2. Sorting 
	2.1 insertion sort
		insertion sort compare the currently selected item with the left side items one by one to findout the location where left side item is <= the current item and right side item is > current item. when the condition match, item will be inserted. so all the left side items (from the lastly picked item) in the list is sorted and the process will repeated for all the rest of the unsorted items.  	
		use case:
		Basically, Insertion sort is efficient for small data values
		Insertion sort is adaptive in nature, i.e. it is appropriate for data sets which are already partially sorted.
		Time complexity : O(n^2)
	2.2 selection sort
	   Selection sort works by repeatedly "selecting" the next-smallest element from the unsorted array and moving it to the front.
	   Time complexity : O(n^2)
	2.3 bubble sort
		works by repeatedly swapping the adjacent elements if they are in the wrong order. This algorithm is not suitable for large data sets as its average and worst-case time complexity is quite high.
		Time complexity : O(n^2)
	2.4 shell sort
		Shell sort is mainly a variation of Insertion Sort. In insertion sort, we move elements only one position ahead. When an element has to be moved far ahead, many movements are involved. The idea of ShellSort is to allow the exchange of far items. In Shell sort, we make the array h-sorted for a large value of h.
		Time complexity : O(n^2)
	2.5 merge sort
		Merge sort works by splitting the input in half, recursively sorting each half, and then merging the sorted halves back together.
		ie. The Merge Sort algorithm is a sorting algorithm that is considered as an example of the divide and conquer strategy. So, in this algorithm, the array is initially divided into two equal halves and then they are combined in a sorted manner. We can think of it as a recursive algorithm that continuously splits the array in half until it cannot be further divided.  i.e. it is the base case to stop the recursion. If the array has multiple elements, we split the array into halves and recursively invoke the merge sort on each of the halves. Finally, when both the halves are sorted, the merge operation is applied. Merge operation is the process of taking two smaller sorted arrays and combining them to eventually make a larger one.
		Time complexity : O(n log n)
	2.6 heap sort
		Heapsort is similar to selection sortâ€”we're repeatedly choosing the largest item and moving it to the end of our array. But we use a heap to get the largest item more quickly.
		Time complexity : O(n log n)
	2.7 quick sort
		Quicksort works by recursively dividing the input into two smaller arrays around a pivot item: one half has items smaller than the pivot, the other has larger items.
		QuickSort is a Divide and Conquer algorithm. It picks an element as a pivot and partitions the given array around the picked pivot. 
		we can pick pivot from first, last, middle or random index. quick sort algorithm will partition the list by putting the pivot element in a current position. ie. all the smaller elements before x and all greater alements after x. all this should be done in linear time. 
		quicksort(arr[], low, high) {
		pivotIndex = partition(arr, low, high)
		quicksort(arr, low, pivotIndex-1)
		quicksort(arr, pivotIndex+1, high)
		}

		Time complexity : 
		            average case : O(n log n)
                   	worst case :	O(n^2)
3. Searching 
   3.1 Linear Search 
   3.2 Binary Search 


sorryting 1 million records - which algoritm better 