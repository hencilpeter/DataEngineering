Question?
how to design table ? which tool can be used ? while designing datawarehouse, how to decide which table can be ingested as it is and which table require re-design ? ER Diagram which tool been used? what are the diagram being used for designing ? what are the docs (e.g data dictionary ) 



sources : https://career.guru99.com/top-50-datawarehousing-questions-answers/

Data warehousing interview questions:

1. What is Datawarehousing?
Datawarehouse is the repository of a data and it is used for Management decision support system. Datawarehouse consists of wide variety of data that has high level of business conditions at a single point in time. it is repository of integrated information which can be available for queries and analysis.
2. What is Business Intelligence?
Also known as DSS – Decision support system which refers to the technologies, application and practices for the collection, integration and analysis of the business related information or data. 
3. Dimension Table?
A dimension table stores attributes, or dimensions, that describe the objects in a fact table. In data warehousing, a dimension is a collection of reference information about a measurable event.
4.  What is Fact Table?
Fact table contains the measurement of business processes, and it contains foreign keys for the dimension tables.
Example – If the business process is manufacturing of bricks. Average number of bricks produced by one person/machine – measure of the business process
5. What are the stages of Datawarehousing?
(i) Operational Systems (e.g marketing, sales, CRM ) (ii) Integration layer (staging. ) (iii) Data Warehouse (iv) Data Marts 

Offline Operational Database : initial stage where data is simply copied to a server from an operating system. so it will not affect the performance later?
Offline Data Warehouse : data is usually copied and pasted from real time data networks into an offline system
Real Time Datawarehouse :
Integrated Datawarehouse :  combines data from several sources into a single, unified warehouse.
6. What is Data Mining?
process of analyzing the data in different dimensions or perspectives and summarizing into a useful information. Can be queried and retrieved the data from database in their own format.
7. What is OLTP?
   On-Line Transaction Processing. it is an application that modifies the data whenever it received and has large number of simultaneous users.
8. What is OLAP?
   Online Analytical Processing. A system which collects, manages, processes multi-dimensional data for analysis and management purposes.
9. What is the difference between OLTP and OLAP?
	OLTP												OLAP
(i)   Data is from original data source					Data is from various data sources
(ii)  Simple queries by users							Complex queries by system
(iii) Normalized small database							De-normalized Large Database
(iv)  Fundamental business tasks						Multi-dimensional business tasks
(v)   More no. of write 								more no. of reads 
(vi)  quick response is essentail                       slow response as quries are complex. 
(vii) RDBMS 											data warehouse 

10. What is ODS?
 Operational Data Store. It is a repository of real time operational data rather than long term trend data.
11. What is the difference between View and Materialized View?
   (i) Views are not stored physically on the disk. On the other hands, Materialized Views are stored on the disc.
   (ii) View can be defined as a virtual table created as a result of the query expression. However, Materialized View is a physical copy, picture or snapshot of the base table.
   (ii) A view is always updated as the query creating View executes each time the View is used. On the other hands, Materialized View is updated manually or by applying triggers to it.
   (iii) Materialized View responds faster than View as the Materialized View is precomputed.
   (iv) Materialized View utilizes the memory space as it stored on the disk whereas, the View is just a display hence it do not require memory space.

12. What is ETL?
   Extract, Transform and Load. (i) reads the data from the specified data source and extracts a desired subset of data.  (ii) transform the data using rules and lookup tables (iii) convert it to a desired state.
13. What is VLDB?
    Very Large Database and its size is set to be more than one terabyte database. These are decision support systems which is used to serve large number of users.
14. What is real-time datawarehousing?
   Captures the business data whenever it occurs. When there is business activity gets completed, that data will be available in the flow and become available for use instantly.
15.[D] What are Aggregate tables?
	Contain the existing warehouse data which has been grouped to certain level of dimensions. It is easy to retrieve data from the aggregated tables than the original table which has more number of records.
    This table reduces the load in the database server and increases the performance of the query.
16. What is factless fact tables?
	are the fact table which doesn’t contain numeric fact column in the fact table.
17.[D] How can we load the time dimension?
	Time dimensions are usually loaded through all possible dates in a year and it can be done through a program. Here, 100 years can be represented with one row per day.
18.[D] What are Non-additive facts?
	facts that cannot be summed up for any of the dimensions present in the fact table. If there are changes in the dimensions, same facts can be useful.
19. What is conformed fact?
	is a table which can be used  across multiple data marts in combined with the multiple fact tables.
20. What is Datamart?
    A Datamart is a specialized version of Datawarehousing and it contains a snapshot of operational data that helps the business people to decide with the analysis of past trends and experiences.
21. What is Active Datawarehousing?
	datawarehouse that enables decision makers within a company or organization to manage customer relationships effectively and efficiently
22. What is the difference between Datawarehouse and OLAP?
	Datawarehouse is a place where the whole data is stored for analyzing, but OLAP is used for analyzing the data, managing aggregations, information partitioning into minor level information.
23. What is ER Diagram?
	Entity-Relationship diagram,  which illustrates the interrelationships between the entities in the database. This diagram shows the structure of each tables and the links between the tables.
24.[D] What are the key columns in Fact and dimension tables?
	(i) Foreign keys of fact tables are the primary keys of the dimension tables.
	[D](ii) Foreign keys of dimension tables are primary keys of entity tables. 
25. What is SCD?
	slowly changing dimensions, and it applies to the cases where record changes over time.
26.[D] What are the types of SCD?
[D]SCD 1 – The new record replaces the original record  (only active snapshot) delete and insert ?
SCD 2 – A new record is added to the existing customer dimension table (history)
SCD 3 – A original data is modified to include new data (update?)
HyBRID SCD (1 & 2) : Hybrid SCDs are a combination of both SCD 1 and SCD 2.
It may happen that in a table, some columns are important and we need to track changes for them i.e., capture the historical data for them whereas in some columns even if the data changes, we do not have to bother.
For such tables, we implement Hybrid SCDs, where in some columns are Type 1 and some are Type 2.

27.[D] What is BUS Schema?
 consists of suite of confirmed dimension and standardized definition if there is a fact tables.
 refer : https://www.careerride.com/Data-warehousing-BUS-schema.aspx
28. What is Star Schema?
	Each dimension in a star schema is represented with only one-dimension table.
	This dimension table contains the set of attributes.
 refer : https://www.tutorialspoint.com/dwh/dwh_schemas.htm  
29. What is Snowflake Schema?
	Some dimension tables in the Snowflake schema are normalized. The normalization splits up the data into additional tables.
30. What is a core dimension?
	Dimension table which is used as dedicated for single fact table or datamart.
31. What is called data cleaning?
	Cleaning of Orphan records, Data breaching business rules, Inconsistent data and missing information in a database.
32. What is Metadata?
	Metadata is defined as data about the data. The metadata contains information like number of columns used, fix width and limited width, ordering of fields and data types of the fields.
33.[D] What are loops in Datawarehousing?
    ( a set of SQL statements are executed repeatedly until a condition is met.  ? )
	 If there is a loop between the tables, then the query generation will take more time and it creates ambiguity. It is advised to avoid loop between the tables.
34. Whether Dimension table can have numeric value?
	Yes, dimension table can have numeric value as they are the descriptive elements of our business.
35. What is the definition of Cube in Datawarehousing?
	Cubes are logical representation of multidimensional data. The edge of the cube has the dimension members,and the body of the cube contains the data values.
36. What is called Dimensional Modelling?
	Dimensional Modeling is a concept which can be used by dataware house designers to build their own datawarehouse. This model can be stored in two types of tables – Facts and Dimension table. Fact table has facts and measurements of the business and dimension table contains the context of measurements.
37.[D] What are the types of Dimensional Modeling?
	Following are the Types of Dimensions in Data Warehouse:
		Conformed Dimension
		Outrigger Dimension
		Shrunken Dimension
		Role-playing Dimension
		Dimension to Dimension Table
		Junk Dimension
		Degenerate Dimension
		Swappable Dimension
		Step Dimension
38.[D] What is surrogate key?
	a substitute for the natural primary key. It is set to be a unique identifier for each row that can be used for the primary key to a table.
	example ?
39.[D] What is the difference between ER Modeling and Dimensional Modeling?
	ER modeling will have logical and physical model but Dimensional modeling will have only Physical model.
	ER Modeling is used for normalizing the OLTP database design whereas Dimensional Modeling is used for de-normalizing the ROLAP and MOLAP design.
40. What are the steps to build the datawarehouse?
Following are the steps to be followed to build the datawaerhouse:
 (i) Gathering business requirements (ii) Identifying the necessary sources (iii) Identifying the facts
 (iv) Defining the dimensions (v) Defining the attributes (vi) Redefine the dimensions and attributes if required
 (vii) Organize the Attribute hierarchy (viii) Define Relationships (ix) Assign unique Identifiers
41. What are the different types of datawarehosuing?
   (i) Enterprise Datawarehousing (ii) Operational Data Store (iii) Data Mart
42. What needs to be done while starting the database?
 (i) Start an Instance (ii) Mount the database (iii) Open the database
43. What needs to be done when the database is shutdown?
 (i) Close the database (ii) Dismount the database (iii) Shutdown the Instance
44. Can we take backup when the database is opened?
	Yes, we can take full backup when the database is opened
45.[D] What is defined as Partial Backup?
	A Partial backup in an operating system is a backup short of full backup and it can be done while the database is opened or shutdown.
46. What is the goal of Optimizer?
	The goal to Optimizer is to find the most efficient way to execute the SQL statements.
47. What is Execution Plan?
 is a plan which is used to the optimizer to select the combination of the steps.
48. What are the approaches used by Optimizer during execution plan?
	(i) Rule Based (ii) Cost Based
49. What are the tools available for ETL?
	(i) Informatica (ii) Data Stage (iii) Oracle (iv) Warehouse Builder (iv) Ab Initio (v) Data Junction
50.What is the difference between metadata and data dictionary?
	Metadata is defined as data about the data. But, Data dictionary contain the information about the project information, graphs, abinito commands and server information.
=========================================================================================================================	
(i) Multidimensional database? 
A multidimensional database (MDB) is a type of database that is optimized for data warehouse and online analytical processing (OLAP) applications. 
MDBs are frequently created using input from existing relational databases. An OLAP application that accesses data from a multidimensional database is known as a multidimensional OLAP (MOLAP) application. The basic actions carried out on a Multidimensional database as a part of OLAP are Roll-up, Drill-down, Slice & Dice, in order to retrieve the required data from the database, where data is organized in the form of multidimensional cubes.

It represents data in the form of data cubes. Data cubes allow to model and view the data from many dimensions and perspectives. It is defined by dimensions and facts and is represented by a fact table. Facts are numerical measures and fact tables contain measures of the related dimensional tables or names of the facts.

Steps involved:
  1. Assembling data from the client : collects correct data from the client
  2. Grouping different segments of the system:  recognizes and classifies all the data to the respective section 
  3. Noticing the different proportions: the main factors are recognized according to the user’s point of view. These factors are also known as “Dimensions”.
  4. Preparing the actual-time factors and their respective qualities: the factors which are recognized in the previous step are used further for identifying the related qualities. These qualities are also known as “attributes” in the database.
  5. Finding the actuality of factors which are listed previously and their qualities: separates and differentiates the actuality from the factors which are collected by it
  6. Building the Schema to place the data, with respect to the information collected from the steps above  : build the final schema. 
e.g https://www.techtarget.com/searchoracle/definition/multidimensional-database#:~:text=A%20multidimensional%20database%20(MDB)%20is,input%20from%20existing%20relational%20databases.
https://www.geeksforgeeks.org/multidimensional-data-model/

Question : simple example in ADSS. positions from SG, HK, BK ? how to represent?


(2) ROLAP vs MOLAP 
	ROLAP 																	MOLAP
	(i)		ROLAP stands for Relational Online Analytical Processing.		While MOLAP stands for Multidimensional Online Analytical Processing.
	(ii)	ROLAP is used for large data volumes.							While it is used for limited data volumes.
	(iii)	The access of ROLAP is slow.									While the access of MOLAP is fast.
	(iv)	In ROLAP, Data is stored in relation tables.					While in MOLAP, Data is stored in multidimensional array.
	(v)		In ROLAP, Data is fetched from data-warehouse.					While in MOLAP, Data is fetched from MDDBs database.
	(vi)	In ROLAP, Complicated sql queries are used.						While in MOLAP, Sparse matrix is used.
	(vii)	In ROLAP, Static multidimensional view of data is created.		While in MOLAP, Dynamic multidimensional view of data is created.
   reference : https://www.geeksforgeeks.org/difference-between-rolap-and-molap/

(3) Active vs Passive Transformation
	Active transformation changes the number of rows number of rows that pass through the mapping. e.g. filter transformation 
	In case of Passive transformation don’t change the number of rows. In passive transformations the number of input and output rows remain the same, only data is modified at row level.
(4) Difference between Kimball and Inmon
	For designing datawarehouse, there are two most common architectures named Kimball and Inmon been used. 
Kimball approach: starts with (i) recognizing the business process and (ii)questions that Dataware house has to answer. These sets of information are being analyzed and then documented well. The Extract Transform Load (ETL) software brings all data from multiple data sources called data marts and then is loaded into a common area called staging. Then this is transformed into an OLAP cube. 
Applications:
(i) Setup and Built are quick. (ii) Generating report against multiple star schema is very successful.
(iii) Database operations are very effective. (iv) Occupies less space in the database and management is easy.

			        etl / DataMart  \
OLTP Datasources -> etl - DataMart  -> Data Warehouse -> OLAP Cube -> Cube Browser -> Reporting layer 
                    etl \ DataMart  /
					
					
Inmon approach:
starts with a corporate data model. This model recognizes key areas and also takes care of customers, products, and vendors. This model serves for the creation of a detailed logical model which is used for major operations. Details and models are then used to develop a physical model. This model is normalized and makes data redundancy less. 

This is a complex model that is difficult to be used for business purposes for which data marts are created and each department is able to use it for their purposes.

Applications:
(i) The data warehouse is very flexible to changes. (ii) Business processes can be understood very easily.
(iii) Reports can be handled across enterprises. (iv) ETL process is very less prone to errors.


								 etl /DataMart  - OLAP Cube  -> Cube Browser    | 
OLTP Datasources -> Data Warehouse 												| ->  Reporting Layer 
								 etl \DataMart  - OLAP Cube -> Cube Browser     | 
								 
Comparison:
Parameters			Kimball													Inmon
Approach			It has a Bottom-Up Approach for implementation.			It has Top-Down Approach for implementation.
Data Integration	It focuses on Individual business areas.				It focuses on Enterprise-wide areas.
Building Time		It is efficient and takes less time.					It is complex and consumes a lot of time.
Cost				It has iterative steps and is cost-effective.			Initial cost is huge and the development cost is low.
Skills Required		It does not need such skills but a generic 				It needs specialized skills to make work.
					team will do the job.
Maintenance			Here maintenance is difficult.							Here maintenance is easy.
Data Model			It prefers data to be in the De-normalized model.		It prefers data to be in a normalized model.
Data Store Systems	In this, source systems are highly stable.				In this, source systems have a high rate of change.
								 
reference : https://www.geeksforgeeks.org/difference-between-kimball-and-inmon/

(5) Agglomerative (bottom to Top) and divisive hierarchical (Top to Bottom)
	Agglomerative hierarchical method : Each object creates its own clusters. The single Clusters are merged to make larger clusters and the process of merging continues until all the singular clusters are merged into one big cluster that consists of all the objects.
	Divisive Hierarchical clustering method :  Works on the top-down approach. In this method all the objects are arranged within a big singular cluster and the large cluster is continuously divided into smaller clusters until each cluster has a single object.
reference: https://benchpartner.com/q/differentiate-agglomerative-and-divisive-hierarchical-clustering

(6) Cluster analysis in DW [D]
	Cluster analysis is used to define the object without giving the class label. It analyzes all the data that is present in the data warehouse and compare the cluster with the cluster that is already running. 
	It performs the task of assigning some set of objects into the groups are also known as clusters. It is used to perform the data mining job using the technique like statistical data analysis. It includes all the information and knowledge around many fields like machine learning, pattern recognition, image analysis and bio-informatics. Cluster analysis performs the iterative process of knowledge discovery and includes trials and failures. It is used with the pre-processing and other parameters as a result to achieve the properties that are desired to be used.

reference : https://www.sawaal.com/data-warehousing-interview-questions/what-is-the-purpose-of-cluster-analysis-in-data-warehousing_7147#:~:text=Cluster%20analysis%20is%20used%20to,cluster%20that%20is%20already%20running.
(7)  chameleon method used in data warehousing? [D]
operates on the sparse graph having nodes: that represent the data items, and edges: representing the weights of the data items.
This representation allows large dataset to be created and operated successfully. The method finds the clusters that are used in the dataset using two phase algorithm.

☛ The first phase consists of the graph partitioning that allows the clustering of the data items into large number of sub-clusters.
☛ Second phase uses an agglomerative hierarchical clustering algorithm to search for the clusters that are genuine and can be combined together with the sub-clusters that are produced.

reference : http://interviewquestionsanswers.org/__Explain-me-why-is-chameleon-method-used-in-data-warehousing
(8)What is active data warehousing?
	An Active data warehouse aims to capture data continuously and deliver real time data. They provide a single integrated view of a customer across multiple business lines. It is associated with Business Intelligence Systems.

(9)Virtual Data Warehousing ?
 It is is an independent compute resource that can be leveraged at any time for SQL execution and DML (Data Manipulation Language) and then turned off when it isn’t needed. For decades, traditional on-premise data warehouses have tightly coupled data storage and compute, making it a challenge to scale either on-demand. However, today’s businesses need to store and analyze vast quantities of both structured and unstructured data from disparate services, necessitating a service that can respond to large data volumes and variable compute needs for applications such as visualization.
 reference : https://www.snowflake.com/data-warehousing-glossary/virtual-warehouse/
 
(10)Snapshot : Snapshot refers to a complete visualization of data at the time of extraction.
(11)XMLA [D]
XML for Analysis (XMLA) is a SOAP-based XML protocol, designed specifically for universal data access to any standard multidimensional data source that can be accessed over an HTTP connection.
(12) ODS (Operational Data Store)
An operational data store usually stores and processes data in real time. An ODS is connected to multiple data sources and pulls data into a central location.
(13)What is a level of Granularity of a fact table?
The granularity is the lowest level of information stored in the fact table. The depth of data level is known as granularity. In date dimension the level could be year, month, quarter, period, week, day of granularity.
reference : https://www.careerride.com/Data-warehousing-Granularity-fact-table.aspx#:~:text=The%20granularity%20is%20the%20lowest,%2C%20week%2C%20day%20of%20granularity.

(14)Junk Dimension ?[D]
Junk Dimension also called as garbage dimension. A garbage dimension is a dimension that consists of low-cardinality columns such as codes, indicators, status, and flags. The garbage dimension is also referred to as a junk dimension. Attributes in a garbage dimension are not related to any hierarchy.
(15)Normalization:
Normalization is the process of reorganizing data in a database so that it meets two basic requirements: There is no redundancy of data, all data is stored in only one place. 
It is also used to eliminate undesirable characteristics like Insertion, Update, and Deletion Anomalies. Normalization divides the larger table into smaller and links them using relationships.
1NF :  it contains no repeating groups or remove repeating groups. 
2NF : It should be in 1NF and all non-key attributes are fully functional dependent on the primary key.
3NF : It should be in 2NF and no transition dependency exists. 
      A transitive dependency refers to some non-prime attribute other than the candidate key that depends on another non-prime attribute that is dependent entirely on the candidate key.
BCNF: BCNF (Boyce Codd Normal Form) is the advanced version of 3NF. A table is in BCNF if every functional dependency X->Y, X is the super key of the table. For BCNF, the table should be in 3NF, and for every FD.
4NF	A relation will be in 4NF if it is in Boyce Codd's normal form and has no multi-valued dependency.
5NF	A relation is in 5NF. If it is in 4NF and does not contain any join dependency, joining should be lossless.

(16)Why do we override the execute method in Struts
	As part of Struts FrameWork we can decvelop the Action Servlet,ActionForm servlets(here ActionServlet means which class extends the Action class is called ActionServlet and ActionFome means which calss extends the ActionForm calss is called the Action Form servlet)and other servlets classes.
	reference : http://dba.fyicenter.com/Interview-Questions/Data-Warehousing/Why_do_we_override_the_execute_method_is_struts_.html
(17) Conformed Dimension [D]
Conformed is the same dimension used in different facts and has the same meaning ex: CustomerID. Role-Playing is the same dimension which used multiple times within the same fact but with different meanings ex: Dat

(18)Explain the ETL cycle's 3-layer architecture
	(i) Staging (ii) Integration (iii) Access Layesr
	
(19)What is data purging?
 Data purging is the process of deleting data from a database. When you purge data, it is permanently erased and cannot be restored. 
(20) Can you define the five main testing phases of a DW project ?
	ETL testing is performed in five stages:
	Identifying data sources and requirements.
	Data acquisition.
	Implement business logic and Dimensional Modeling.
	Build and populate data.
	Build Reports.
(21)[D]What do you mean by the Slice action ? How many slice-operated dimensions are used ?
	Filtration process

(22) What do you mean by threshold value validation
	The Threshold Validation job is used to evaluate configured thresholds (threshold setup) and send out the appropriate alerts from the application.
(22) 3 approache that could be followed for system integration Testing
	TD :  ETL > Validate > DW > DM
	BU :  ETL > Validate > DM > DW
	Hybrid
(23) A database schema 
	 - A database schema is a blueprint or architecture of how our data will look. It doesn’t hold data itself, but instead describes the shape of the data and how it might relate to other tables or models. 
     - is an abstract design that represents the storage of your data in a database. It describes both the organization of data and the relationships between tables in a given database. Developers plan a database schema in advance so they know what components are necessary and how they will connect to each other.
	reference : https://www.educative.io/blog/what-are-database-schemas-examples
(24)How do you prevent data loss
	(i) data backup in regular interval (ii) Keep your computer clean and dust-free. (iii) Keep your anti-virus software up to date. (iv) Create an image backup BEFORE the first signs of hard drive failure.
(25)Explain logshipping
SQL Server Log shipping allows you to automatically send transaction log backups from a primary database on a primary server instance to one or more secondary databases on separate secondary server instances.
(26)Describe the knowledge of cloud database
	Db services on cloud platform
	No Physical space required
	It's highly secure and can be accessed simply using the webinterface
(27)ODBC : 
Open Database Connectivity (ODBC) is an open standard application programming interface (API) that allows application programmers to access any database.
(28)DBA day to day activity
	Monitoring user access and security
	Monitoring Sever health
	Monitoring DB performance and Installing 
	Monitoring new bersion of the databse
	prompt response to end users
(29)Troubleshoot database problems
	Ticketing system	
(30)datawarehouse schema design [D] 
     Schema is a logical description of the entire database. It includes the name and description of records of all record types including all associated data-items and aggregates. Much like a database, a data warehouse also requires to maintain a schema.

(31) What is Database Entity? [D]
	Database entity is a thing, person, place, unit, object or any item about which the data should be captured and stored in the form of properties, workflow and tables.

	Think about the Entities
	List out the entities
	A simple star Model
	
Think about the relationships
	Think about the missing relationship

Think about the question you want to answer
	what menu